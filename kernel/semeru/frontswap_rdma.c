/**
 * frontswap_rdma is the RDMA component for frontswap path.
 * Its RDMA queues are different from the swap version.
 * 
 * 
 * 1. Global variables
 * 
 * 2. Structure for RDMA connection
 * 	One session for each Memory server.
 * 		
 * 
 * 	2.1 Structure of RDMA session
 * 			1 RDMA queue for Data/Control path
 * 
 * 	2.2	Structure of RDMA queue
 * 			1 cm_id
 * 			1 QP
 * 			1 IB_POLL_DIRECT CQ for control/data path
 * 			
 * 	2.3 There are 3 types of communication here
 * 			a. Data Path. Triggered by swap
 * 					=> all the rdma_queues[]
 * 			b. Control Path. Invoked by User space.
 * 					=> rdma_queue[0]
 * 			c. Exchange meta data with memory server. 2-sided RDMA and only used during initializaiton.
 * 					=> rdma_queue[0]
 * 
 */


// Semeru
#include "frontswap_path.h"
#include "semeru_cpu.h"

//
// Implement the global vatiables here
//

struct rdma_session_context *rdma_session_global_ptr;
int online_cores; // Control the parallelism

//debug
u64 rmda_ops_count = 0;
u64 cq_notify_count = 0;
u64 cq_get_count = 0;

// 
// static functions
//




/**
 * Declared in swap_global_struct_bd_layer.h
 * 
 * Get the pte_t value  of the user space virtual address.
 * For the kernel space virtual address, allocated by kmalloc or kzalloc, user the virt_to_phys is good.
 * 
 * 1) This is a 5 level PageTable, since 4.11-rc2.
 * 		check https://lwn.net/Articles/717293/
 *    
 * 		7 bits are discarded.
 *    9 bits each level.
 * 
 *          size per entry.
 *    pgd : 256TB
 * 		p4d : 512GB
 *    pud : 1GB 
 * 		pmd : 2MB,   // huge page bit.
 * 		pte : 4KB
 * 
 * Modify the transparent hugepage, /sys/kernel/mm/transparent_hugepage/enabled to madvise.
 * to always [madvise] never
 * OR we have to delete the last level pte.
 * 
 * [XX] If too many pages need to be walked, switch to huge page.
 * 			Or this function will cause significantly overhead.
 * 
 * 
 */
static pte_t *walk_page_table(struct mm_struct *mm, uint64_t addr)
{
	pgd_t *pgd;
	p4d_t *p4d;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	pgd = pgd_offset(mm, addr);

	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd))) // [?] What's the purpose of bad bit ?
		return NULL;

	p4d = p4d_offset(pgd, addr);
	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))
		return NULL;

	pud = pud_offset(p4d, addr);
	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
		return NULL;

	pmd = pmd_offset(pud, addr);
	if (pmd_none(*pmd))
		return NULL;

	ptep = pte_offset_map(pmd, addr);

	return ptep;
}

//
// >>>>>>>>>>>>>>>>>>>>>>  Start of handle  TWO-SIDED RDMA message section >>>>>>>>>>>>>>>>>>>>>>
//

/**
 * Received the CQE for the posted recv_wr
 *  
 */
void two_sided_message_done(struct ib_cq *cq, struct ib_wc *wc)
{
	struct semeru_rdma_queue *rdma_queue = cq->cq_context;
	int ret = 0;

	// 1) check the wc status
	if (wc->status != IB_WC_SUCCESS) {
		printk(KERN_ERR "%s, cq completion failed with wr_id 0x%llx status %d,  status name %s, opcode %d,\n",
		       __func__, wc->wr_id, wc->status, rdma_wc_status_name(wc->status), wc->opcode);

		goto out;
	}

	// 2) process the received message
	switch (wc->opcode) {
	case IB_WC_RECV:
		// Recieve 2-sided RDMA recive wr
		printk("%s, Got a WC from CQ, IB_WC_RECV. \n", __func__);

		// Need to do actions based on the received message type.
		ret = handle_recv_wr(rdma_queue, wc);
		if (unlikely(ret)) {
			printk(KERN_ERR "%s, recv wc error: %d\n", __func__, ret);
			goto out;
		}
		break;

	case IB_WC_SEND:

		printk("%s, Got a WC from CQ, IB_WC_SEND. 2-sided RDMA post done. \n", __func__);
		break;

	default:
		printk(KERN_ERR "%s:%d Unexpected opcode %d, Shutting down\n", __func__, __LINE__, wc->opcode);
		goto out;
	} // switch

	//decrease the counter
	atomic_dec(&rdma_queue->rdma_post_counter);

out:
	return;
}

/**
 * Send a RDMA message to remote server.
 * Used for RDMA conenction build.
 * 
 */
int send_message_to_remote(struct rdma_session_context *rdma_session, int rdma_queue_ind , int messge_type  , int chunk_num)
{
	int ret = 0;
	struct ib_recv_wr * recv_bad_wr;
	struct ib_send_wr * send_bad_wr;
	struct semeru_rdma_queue *rdma_queue;

	rdma_queue = &(rdma_session->rdma_queues[rdma_queue_ind]);
	rdma_session->rdma_send_req.send_buf->type = messge_type;
	rdma_session->rdma_send_req.send_buf->mapped_chunk = chunk_num; 		// 1 Meta , N-1 Data Regions


	// post a 2-sided RDMA recv wr first.
	ret = ib_post_recv(rdma_queue->qp, &rdma_session->rdma_recv_req.rq_wr, &recv_bad_wr);
	if(ret) {
		printk(KERN_ERR "%s, Post 2-sided message to receive data failed.\n", __func__);
		goto err;
	}
	atomic_inc(&rdma_queue->rdma_post_counter);


	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk("Send a Message to memory server. send_buf->type : %d, %s \n", messge_type, rdma_message_print(messge_type) );
	#endif

	ret = ib_post_send(rdma_queue->qp, &rdma_session->rdma_send_req.sq_wr, &send_bad_wr);
	if (ret) {
		printk(KERN_ERR "%s: BIND_SINGLE MSG send error %d\n", __func__, ret);
		goto err;
	}
	atomic_inc(&rdma_queue->rdma_post_counter);
	// #if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	// else{
	// 	printk("%s: 2-sided RDMA message[%llu] send. \n",__func__,rmda_ops_count++);
	// }
	// #endif

err:
	return ret;	
}

/**
 * Receive a WC, IB_WC_RECV.
 * Read the data from the posted WR.
 * 		For this WR, its associated DMA buffer is rdma_session_context->recv_buf.
 * 
 * Action 
 * 		According to the RDMA message information, rdma_session_context->state, to set some fields.
 * 		FREE_SIZE : set the 
 * 
 * More Explanation
 * 		For the 2-sided RDMA, the receiver even can not responds any message back ?
 * 		The character of 2-sided RDMA communication is just to send a interrupt to receiver's CPU.
 * 
 */
int handle_recv_wr(struct semeru_rdma_queue *rdma_queue, struct ib_wc *wc)
{
	int ret = 0;
	struct rdma_session_context *rdma_session = rdma_queue->rdma_session;

	if (wc->byte_len != sizeof(struct message)) { // Check the length of received message
		printk(KERN_ERR "%s, Received bogus data, size %d\n", __func__, wc->byte_len);
		ret = -1;
		goto out;
	}

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	// Is this check necessary ??
	if (unlikely(rdma_queue->state < CONNECTED)) {
		printk(KERN_ERR "%s, RDMA is not connected\n", __func__);
		return -1;
	}
#endif

	switch (rdma_session->rdma_recv_req.recv_buf->type) {
	case AVAILABLE_TO_QUERY:
#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk("%s, Received AVAILABLE_TO_QERY, memory server is prepared well. We can qu\n ", __func__);
#endif

		rdma_queue->state = MEMORY_SERVER_AVAILABLE;

		wake_up_interruptible(&rdma_queue->sem);
		break;

	case FREE_SIZE:

		// get the number of Free Regions.
		printk(KERN_INFO "%s, Received FREE_SIZE, avaible chunk number : %d \n ", __func__,
		       rdma_session->rdma_recv_req.recv_buf->mapped_chunk);

		rdma_session->remote_chunk_list.chunk_num = rdma_session->rdma_recv_req.recv_buf->mapped_chunk;
		rdma_queue->state = FREE_MEM_RECV;

		ret = init_remote_chunk_list(rdma_session);
		if (unlikely(ret)) {
			printk(KERN_ERR "Initialize the remote chunk failed. \n");
		}

		wake_up_interruptible(&rdma_queue->sem);

		break;
	case GOT_CHUNKS:

		// Got memory chunks from remote memory server, do contiguous mapping.
		bind_remote_memory_chunks(rdma_session);

		// Received free chunks from remote memory,
		// Wakeup the waiting main thread and continure.
		rdma_queue->state = RECEIVED_CHUNKS;
		wake_up_interruptible(&rdma_queue->sem); // Finish main function.

		break;

	default:
		printk(KERN_ERR "%s, Recieved WRONG RDMA message %d \n", __func__,
		       rdma_session->rdma_recv_req.recv_buf->type);
		ret = -1;
		goto out;
	}

out:
	return ret;
}

/**
 * Check the available memory size on this session/memory server
 * All the QPs of the session share the same memory, request for any QP is good.
 * 
 */
int semeru_query_available_memory(struct rdma_session_context *rdma_session)
{
	int ret = 0;
	struct semeru_rdma_queue *rdma_queue;

	// Memory servers use the last conencted QP to pass the available memory info.
	rdma_queue = &(rdma_session->rdma_queues[online_cores - 1]);

	//wait untile all the rdma_queue are connected.
	// wait for the last rdma_queue[online_cores-1] get MEMORY_SERVER_AVAILABLE
	wait_event_interruptible(rdma_queue->sem, rdma_queue->state == MEMORY_SERVER_AVAILABLE);
	printk(KERN_INFO "%s, All %d rdma_queues are prepared well. Query its available memory. \n", __func__,
	       online_cores);

	// And then let client use the rdma_queue[0] for communication.
	// Post a 2-sided RDMA to query memory server's available memory.
	rdma_queue = &(rdma_session->rdma_queues[0]);
	ret = send_message_to_remote(rdma_session, 0, QUERY, 0); // for QERY, chunk_num = 0
	if (ret) {
		printk(KERN_ERR "%s, Post 2-sided message to remote server[%d] failed.\n", 
			__func__, rdma_session->mem_server_id);
		goto err;
	}

	// recive 2-sided wr.
	drain_rdma_queue(rdma_queue);

	// Sequence controll
	wait_event_interruptible(rdma_queue->sem, rdma_queue->state == FREE_MEM_RECV);

	printk("%s: Got %d free memory chunks from remote memory server[%d]. Request for Chunks \n", __func__,
	       rdma_session->remote_chunk_list.chunk_num, rdma_session->mem_server_id);

err:
	return ret;
}

/**
 *  Post a 2-sided request for chunk mapping.
 * 
 * 
 * 
 * More Explanation
 * 	We can only post a cqe notification per time, OR these cqe notifications may overlap and lost.
 * 	So we post the cqe notification in cq_event_handler function.
 * 
 */
int semeru_requset_for_chunk(struct rdma_session_context *rdma_session, int num_chunk)
{
	int ret = 0;
	struct semeru_rdma_queue *rdma_queue;

	// We can request the registerred memory from any QP of the target memory server.
	rdma_queue = &(rdma_session->rdma_queues[0]); 
	if (num_chunk == 0 || rdma_session == NULL) {
		printk(KERN_ERR "%s, current memory server has no available memory at all. Exit. \n", __func__);
		goto err;
	}

	// Post a 2-sided RDMA to requst all the available regions from current memory server.
	ret = send_message_to_remote(rdma_session, 0, REQUEST_CHUNKS, num_chunk);
	if (ret) {
		printk(KERN_ERR "%s, Post 2-sided message to remote server[%d] failed.\n", 
			__func__, rdma_session->mem_server_id);
		goto err;
	}

	drain_rdma_queue(rdma_queue);

	// Sequence controll
	wait_event_interruptible(rdma_queue->sem, rdma_queue->state == RECEIVED_CHUNKS);

	printk(KERN_INFO "%s, Got %d chunks from memory server[%d].\n", 
		__func__, num_chunk, rdma_session->mem_server_id);

err:
	return ret;
}

//
// <<<<<<<<<<<<<<  End of handling TWO-SIDED RDMA message section <<<<<<<<<<<<<<
//








//
// ############# Start of RDMA Communication (CM) event handler ########################
//
// [?] All the RDMA Communication(CM) event is triggered by hardware and mellanox driver.
//		No need to maintain a daemon thread to handle the CM events ?  -> stateless handler 
//


/**
 * The rdma CM event handler function
 * 
 * [?] Seems that this function is triggered when a CM even arrives this device. 
 * 
 * More Explanation
 * 	 CMA Event handler  && cq_event_handler , 2 different functions for CM event and normal RDMA message handling.
 * 
 */
int semeru_rdma_cm_event_handler(struct rdma_cm_id *cma_id, struct rdma_cm_event *event)
{
	int ret = 0;
	struct semeru_rdma_queue *rdma_queue = cma_id->context;


	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	pr_info("cma_event type %d, type_name: %s \n", event->event, rdma_cm_message_print(event->event));
	#endif

	switch (event->event) {
	case RDMA_CM_EVENT_ADDR_RESOLVED:
		rdma_queue->state = ADDR_RESOLVED;

		#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk("%s,  get RDMA_CM_EVENT_ADDR_RESOLVED. Send RDMA_ROUTE_RESOLVE to Memory server \n",__func__);
		#endif 

		// Go to next step directly, resolve the rdma route.
		ret = rdma_resolve_route(cma_id, 2000);
		if (ret) {
			printk(KERN_ERR "%s,rdma_resolve_route error %d\n", __func__, ret);
		}
		break;

	case RDMA_CM_EVENT_ROUTE_RESOLVED:

		#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		// RDMA route is solved, wake up the main process  to continue.
    	printk("%s : RDMA_CM_EVENT_ROUTE_RESOLVED, wake up rdma_queue->sem\n ",__func__);
		#endif	

		// Sequencial controll 
		rdma_queue->state = ROUTE_RESOLVED;
		wake_up_interruptible(&rdma_queue->sem);
		break;

	case RDMA_CM_EVENT_CONNECT_REQUEST:		// Receive RDMA connection request

    printk("Receive but Not Handle : RDMA_CM_EVENT_CONNECT_REQUEST \n");
		break;

	case RDMA_CM_EVENT_ESTABLISHED:
	  printk("%s, ESTABLISHED, wake up rdma_queue->sem\n", __func__);

		rdma_queue->state = CONNECTED;
    wake_up_interruptible(&rdma_queue->sem);		

		break;

	case RDMA_CM_EVENT_ADDR_ERROR:
	case RDMA_CM_EVENT_ROUTE_ERROR:
	case RDMA_CM_EVENT_CONNECT_ERROR:
	case RDMA_CM_EVENT_UNREACHABLE:
	case RDMA_CM_EVENT_REJECTED:
		printk(KERN_ERR "%s, cma event %d, event name %s, error code %d \n", __func__, event->event,
														rdma_cm_message_print(event->event), event->status);
		rdma_queue->state = ERROR;
		wake_up_interruptible(&rdma_queue->sem);
		break;

	case RDMA_CM_EVENT_DISCONNECTED:	//should get error msg from here
		printk( "%s, Receive DISCONNECTED  signal \n",__func__);

		if(rdma_queue->freed){ // 1, during free process.
			
			// freed >= 1
			//  Client request for RDMA disconnection.
			printk("%s, rdma_queue[%d] RDMA disconnect event, requested by client. \n",
					__func__, rdma_queue->q_index);

		}else{	
			// freed ==0,
			// Remote server requests for disconnection.
			// e.g., the memory server is crashed.
			printk("%s, rdma_queue[%d] RDMA disconnect event, requested by memory server. \n",
					__func__, rdma_queue->q_index);
			
			// Because the QP is crashed, no need to send the disconnect request anymore
			// Just wait for the RDMA_CM_EVENT_TIMEWAIT_EXIT signal
			// When receive the RDMA_CM_EVENT_DISCONNECTED signal, 
			// All the QP are disconnected already.

			rdma_queue->freed = 255; // one of memory servers crashed
		}
		break;

	case RDMA_CM_EVENT_TIMEWAIT_EXIT:
		// After the received DISCONNECTED_EVENT, need to wait the on-the-fly RDMA message
		// When receive the RDMA_CM_EVENT_TIMEWAIT_EXIT signal, it means all the on-the-fly messages are flushed.
		// https://linux.die.net/man/3/rdma_get_cm_event

		pr_warn("%s, Wait for on-the-fly RDMA message finished. \n",__func__);
		rdma_queue->state = CM_DISCONNECT;

		//Wakeup caller
		if(rdma_queue->freed == 255){
			pr_warn("%s, rdma_queue[%d] one of memory servers crashed. nothing to wakeup.", 
					__func__, rdma_queue->q_index );
			break;
		}
		wake_up_interruptible(&rdma_queue->sem);

		break;

	case RDMA_CM_EVENT_DEVICE_REMOVAL:	//this also should be treated as disconnection, and continue disk swap
		printk(KERN_ERR "%s, cma detected device removal!!!!\n", __func__);
		ret = -1;
		break;

	default:
		printk(KERN_ERR "%s,oof bad type!\n",__func__);
		wake_up_interruptible(&rdma_queue->sem);
		break;
	}

	return ret;
}




// Resolve the destination IB device by the destination IP.
// [?] Need to build some route table ?
// 
static int rdma_resolve_ip_to_ib_device(struct rdma_session_context *rdma_session, struct semeru_rdma_queue *rdma_queue ){

	int ret;
	struct sockaddr_storage sin; 
	struct sockaddr_in *sin4 = (struct sockaddr_in *)&sin;  


	sin4->sin_family = AF_INET;
	memcpy((void *)&(sin4->sin_addr.s_addr), rdma_session->addr, 4);   	// copy 32bits/ 4bytes from cb->addr to sin4->sin_addr.s_addr
	sin4->sin_port = rdma_session->port;                             		// assign cb->port to sin4->sin_port


	ret = rdma_resolve_addr(rdma_queue->cm_id, NULL, (struct sockaddr *)&sin, 2000); // timeout time 2000ms 
	if (ret) {
		printk(KERN_ERR "%s, rdma_resolve_ip_to_ib_device error %d\n", __func__, ret);
		return ret;
	}else{
		printk("rdma_resolve_ip_to_ib_device - rdma_resolve_addr success.\n");
	}
	
	// Wait for the CM events to be finished:  handled by rdma_cm_event_handler()
	// 	1) resolve addr
	//	2) resolve route
	// Come back here and continue:
	//
	wait_event_interruptible(rdma_queue->sem, rdma_queue->state >= ROUTE_RESOLVED);   //[?] Wait on cb->sem ?? Which process will wake up it.
	if (rdma_queue->state != ROUTE_RESOLVED) {
		printk(KERN_ERR  "%s, addr/route resolution did not resolve: state %d\n", __func__, rdma_queue->state);
		return -EINTR;
	}

	printk(KERN_INFO "%s, resolve address and route successfully\n", __func__);
	return ret;
}



/**
 * Build the Queue Pair (QP).
 * 
 */
int semeru_create_qp(struct rdma_session_context *rdma_session, struct semeru_rdma_queue * rdma_queue){
	struct ib_qp_init_attr init_attr;
	int ret;

	memset(&init_attr, 0, sizeof(init_attr));
	init_attr.cap.max_send_wr = rdma_session->send_queue_depth; 
	init_attr.cap.max_recv_wr = rdma_session->recv_queue_depth;  
	//init_attr.cap.max_recv_sge = MAX_REQUEST_SGL;					// enable the scatter
	init_attr.cap.max_recv_sge = 1;		// for the receive, no need to enable S/G.
	init_attr.cap.max_send_sge = MAX_REQUEST_SGL;					// enable the gather 
	init_attr.sq_sig_type = IB_SIGNAL_REQ_WR;     // Receive a signal when posted wr is done.
	init_attr.qp_type = IB_QPT_RC;                // Queue Pair connect type, Reliable Communication.  [?] Already assign this during create cm_id.

	// Both recv_cq and send_cq use the same cq.
	init_attr.send_cq = rdma_queue->cq;
	init_attr.recv_cq = rdma_queue->cq;

	ret = rdma_create_qp(rdma_queue->cm_id, rdma_session->rdma_dev->pd, &init_attr);
	if (!ret){
		// Record this queue pair.
		rdma_queue->qp = rdma_queue->cm_id->qp;
  	}else{
    	printk(KERN_ERR "%s:  Create QP falied. errno : %d \n", __func__, ret);
  	}

	return ret;
}

// Prepare for building the Connection to remote IB servers.
// Create Queue Pair : pd, cq, qp,
int semeru_create_rdma_queue(struct rdma_session_context *rdma_session, int rdma_queue_index)
{
	int ret = 0;
	struct rdma_cm_id *cm_id;
	struct semeru_rdma_queue *rdma_queue;
	int comp_vector = 0; // used for IB_POLL_DIRECT

	rdma_queue = &(rdma_session->rdma_queues[rdma_queue_index]);
	cm_id = rdma_queue->cm_id;

	// 1) Build PD.
	// flags of Protection Domain, (ib_pd) : Protect the local OR remote memory region.  [??] the pd is for local or for the remote attached to the cm_id ?
	// Local Read is default.
	// Allocate and initialize for one rdma_session should be good.
	if (rdma_session->rdma_dev == NULL) {
		rdma_session->rdma_dev = kzalloc(sizeof(struct semeru_rdma_dev), GFP_KERNEL);

		rdma_session->rdma_dev->pd = ib_alloc_pd(
			cm_id->device,
			IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_READ |
				IB_ACCESS_REMOTE_WRITE); // No local read ??  [?] What's the cb->pd used for ?
		rdma_session->rdma_dev->dev = rdma_session->rdma_dev->pd->device;

		if (IS_ERR(rdma_session->rdma_dev->pd)) {
			printk(KERN_ERR "%s, ib_alloc_pd failed\n", __func__);
			goto err;
		}
		printk(KERN_INFO "%s, created pd %p\n", __func__, rdma_session->rdma_dev->pd);

		// Time to reserve RDMA buffer for this session.
		setup_rdma_session_commu_buffer(rdma_session);
	}

	// 2) Build CQ
	rdma_queue->cq = ib_alloc_cq(cm_id->device, rdma_queue,
				     (rdma_session->send_queue_depth + rdma_session->recv_queue_depth), comp_vector,
				     IB_POLL_DIRECT);
	if (IS_ERR(rdma_queue->cq)) {
		printk(KERN_ERR "%s, ib_create_cq failed\n", __func__);
		ret = PTR_ERR(rdma_queue->cq);
		goto err;
	}
	printk(KERN_INFO "%s, created cq %p\n", __func__, rdma_queue->cq);

	// 3) Build QP.
	ret = semeru_create_qp(rdma_session, rdma_queue);
	if (ret) {
		printk(KERN_ERR "%s, failed: %d\n", __func__, ret);
		goto err;
	}
	printk(KERN_INFO "%s, created qp %p\n", __func__, rdma_queue->qp);

err:
	return ret;
}

/**
 * Reserve two RDMA wr for receive/send meesages
 * 		rdma_session_context->rq_wr
 * 		rdma_session_context->send_sgl
 * Post these 2 WRs to receive/send controll messages.
 */
void octopus_setup_message_wr(struct rdma_session_context *rdma_session)
{
	// 1) Reserve a wr for 2-sided RDMA recieve wr
	rdma_session->rdma_recv_req.recv_sgl.addr 	= rdma_session->rdma_recv_req.recv_dma_addr;  // sg entry addr    
	rdma_session->rdma_recv_req.recv_sgl.length = sizeof(struct message);				// address of the length
	rdma_session->rdma_recv_req.recv_sgl.lkey = rdma_session->rdma_dev->dev->local_dma_lkey;

	rdma_session->rdma_recv_req.rq_wr.sg_list = &(rdma_session->rdma_recv_req.recv_sgl);
	rdma_session->rdma_recv_req.rq_wr.num_sge = 1;  // ib_sge scatter-gather's number is 1
	rdma_session->rdma_recv_req.cqe.done = two_sided_message_done;
	rdma_session->rdma_recv_req.rq_wr.wr_cqe = &(rdma_session->rdma_recv_req.cqe);


	// 2) Reserve a wr for 2-sided RDMA send wr
	rdma_session->rdma_send_req.send_sgl.addr = rdma_session->rdma_send_req.send_dma_addr;
	rdma_session->rdma_send_req.send_sgl.length = sizeof(struct message);
	rdma_session->rdma_send_req.send_sgl.lkey = rdma_session->rdma_dev->dev->local_dma_lkey;

	rdma_session->rdma_send_req.sq_wr.opcode = IB_WR_SEND;		// ib_send_wr.opcode , passed to wc.
	rdma_session->rdma_send_req.sq_wr.send_flags = IB_SEND_SIGNALED;
	rdma_session->rdma_send_req.sq_wr.sg_list = &rdma_session->rdma_send_req.send_sgl;
	rdma_session->rdma_send_req.sq_wr.num_sge = 1;
	rdma_session->rdma_send_req.cqe.done = two_sided_message_done;
	rdma_session->rdma_send_req.sq_wr.wr_cqe = &(rdma_session->rdma_send_req.cqe);

	return;
}



/**
 * We reserve two WRs for send/receive RDMA messages in a 2-sieded way.
 * 	a. Allocate 2 buffers
 * 		dma_session->recv_buf
 * 		rdma_session->send_buf
 *	b. Bind their DMA/BUS address to  
 * 		rdma_context->recv_sgl
 * 		rdma_context->send_sgl
 * 	c. Bind the ib_sge to send/receive WR
 * 		rdma_context->rq_wr
 * 		rdma_context->sq_wr
 */
int semeru_setup_buffers(struct rdma_session_context *rdma_session){
	int ret;

	// 1) Allocate some DMA buffers.
	// [x] Seems that any memory can be registered as DMA buffers, if they satisfy the constraints:
	// 1) Corresponding physical memory is allocated. The page table is built. 
	//		If the memory is allocated by user space allocater, malloc, we need to walk  through the page table.
	// 2) The physial memory is pinned, can't be swapt/paged out.
  rdma_session->rdma_recv_req.recv_buf = kzalloc(sizeof(struct message), GFP_KERNEL);  	//[?] Or do we need to allocate DMA memory by get_dma_addr ???
	rdma_session->rdma_send_req.send_buf = kzalloc(sizeof(struct message), GFP_KERNEL);  

	// Get DMA/BUS address for the receive buffer
	rdma_session->rdma_recv_req.recv_dma_addr = ib_dma_map_single(rdma_session->rdma_dev->dev, rdma_session->rdma_recv_req.recv_buf, 
																																	sizeof(struct message), DMA_BIDIRECTIONAL);
	rdma_session->rdma_send_req.send_dma_addr = ib_dma_map_single(rdma_session->rdma_dev->dev, rdma_session->rdma_send_req.send_buf, 
																																	sizeof(struct message), DMA_BIDIRECTIONAL);	

	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk("%s, Got dma/bus address 0x%llx, for the recv_buf 0x%llx \n", __func__, (unsigned long long)rdma_session->rdma_recv_req.recv_dma_addr, 
																			(unsigned long long)rdma_session->rdma_recv_req.recv_buf);
	printk("%s, Got dma/bus address 0x%llx, for the send_buf 0x%llx \n", __func__, (unsigned long long)rdma_session->rdma_send_req.send_dma_addr, 
																			(unsigned long long)rdma_session->rdma_send_req.send_buf);
	#endif

	

	// 3) Add the allocated (DMA) buffer to reserved WRs
	octopus_setup_message_wr(rdma_session);
	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk(KERN_INFO "%s, allocated & registered buffers...\n", __func__);
	#endif


	// 4) Initialzation for the Data-Path
	rdma_session->write_tag = kzalloc(sizeof(uint32_t), GFP_KERNEL);
	*(rdma_session->write_tag) = 0; 	// initialize its value to 0. | -- 16 bits, dirty or not --| -- 16bits version --|
	rdma_session->write_tag_dma_addr = ib_dma_map_single(rdma_session->rdma_dev->dev, rdma_session->write_tag, sizeof(uint32_t), DMA_TO_DEVICE);

	//		Initialize the wr here to save some RDMA write issuing time.
	ret = init_write_tag_rdma_command(rdma_session);
	if(unlikely(ret != 0)){
		printk(KERN_ERR "%s, initialize write_tag rdma command failed. \n",__func__);
		goto err;
	}


	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk(KERN_INFO "%s is done. \n", __func__);
	#endif


err:
	return ret;
}



/**
 * All the PD, QP, CP are setted up, connect to remote memory servers.
 * This will send a CM event to remote IB server && get a CM event response back.
 * Each QP needs to build a connection with the remote memory server.
 */
int semeru_connect_remote_memory_server(struct rdma_session_context *rdma_session, int rdma_queue_inx ){
	struct rdma_conn_param conn_param;
	int ret;
	struct semeru_rdma_queue * rdma_queue;
	struct ib_recv_wr *bad_wr;

	
	rdma_queue = &(rdma_session->rdma_queues[rdma_queue_inx]);
	memset(&conn_param, 0, sizeof conn_param);
	conn_param.responder_resources = 1;
	conn_param.initiator_depth = 1;
	conn_param.retry_count = 10;

	// After rdma connection built, memory server will send a 2-sided RDMA message immediately
	// post a recv on cq to wait wc
	// MT safe during the connection process
	atomic_inc(&rdma_queue->rdma_post_counter);
	ret = ib_post_recv(rdma_queue->qp, &rdma_session->rdma_recv_req.rq_wr, &bad_wr); 
	if(ret){
		printk(KERN_ERR "%s: post a 2-sided RDMA message error \n",__func__);
		goto err;
	}	

	ret = rdma_connect(rdma_queue->cm_id, &conn_param);  // RDMA CM event 
	if (ret) {
		printk(KERN_ERR "%s, rdma_connect error %d\n", __func__, ret);
		return ret;
	}
	printk(KERN_INFO "%s, rdma_queue[%d] Send RDMA connect request to remote server[%d] \n", 
			__func__, rdma_queue_inx, rdma_session->mem_server_id);

	// Get free memory information from Remote Mmeory Server
	// [X] After building the RDMA connection, server will send its free memory to the client.
	// Post the WR to get this RDMA two-sided message.
	// When the receive WR is finished, cq_event_handler will be triggered.
	
	// a. post a receive wr

	// b. Request a notification (IRQ), if an event arrives on CQ entry.
	// For a IB_POLL_DIRECT cq, we need to poll the cqe manually.

	// After receiving the CONNECTED state, means the RDMA connection is built.
	// Prepare to receive 2-sided RDMA message
	wait_event_interruptible(rdma_queue->sem, rdma_queue->state >= CONNECTED);
	if (rdma_queue->state == ERROR) {
		printk(KERN_ERR "%s, Received ERROR response, state %d\n", __func__, rdma_queue->state);
		return -1;
	}

	// for the posted ib_recv_wr.
	// check available memory
	drain_rdma_queue(rdma_queue);

	// printk(KERN_INFO "%s, rdma_queue[%d] connectted to memory server[%d] successful\n", 
	// 	__func__, rdma_queue_inx, rdma_session->mem_server_id);

err:
	return ret;
}


//
// <<<<<<<<<<<<<<<<<<<<<<<  End of RDMA Communication (CM) event handler <<<<<<<<<<<<<<<<<<<<<<<
//






//
// >>>>>>>>>>>>>>>  Start of ONE-SIDED RDMA message section >>>>>>>>>>>>>>>
//	Stateful,
//		Need to allocate and maintain some resource.
//		i.e. The 1-sided WR.
//		All the reource stored in rmem_rdma_queue, mapped to each dispatch queue.
//		These resources will be invoked by the Disk Driver via the definination of blk_mq_ops->.queue_rq.
//
//  [?] Need a daemon thread ? 
//		Seems no. After getting the chunk remote_addr/rkey, no need to wait any signals.
//		The action is triggered by blk_mq_ops->.queue_rq, which pops i/o requset.
//







/**
 * Initialize the Data-Path, write tag rdma command  
 *  
 * [x] One write_tag wr with one RDMA buffer, can be writting to all the different destinations,Regions.
 * 
 */
int init_write_tag_rdma_command(struct rdma_session_context *rdma_session){
	int ret = 0; // if success, return 0.

/*

	// Allocate 
	// [XX] Definitely not need scatter-gather for the write_tag mechanism.
	//      Because all the 
	rdma_session->write_tag_rdma_cmd = (struct semeru_rdma_req_sg*)kzalloc( sizeof(struct semeru_rdma_req_sg) + 1 * sizeof(struct scatterlist)  \
																																, GFP_KERNEL );
	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	if(unlikely(rdma_session->write_tag_rdma_cmd == NULL)){
		printk(KERN_ERR "%s, allocate write_tag_rdma_cmd failed. \n",__func__);
		goto err;
	}
	#endif

	// 1) Do initialization for some fiels
	rdma_session->write_tag_rdma_cmd->io_rq 		= NULL;  // Always null ? seems don't need this field, even this is Data-Path.


	// 2) Local RDMA buffers

	// the vlaue of rdma_session->write_tag needs to be dynamically assigned before sending.

	//	Each 1-sided RDMA write can only write to 1 Region, only reserve 1 write_tag 
	rdma_session->write_tag_rdma_cmd->sge_list[0].addr 		= rdma_session->write_tag_dma_addr;  // source addr
	rdma_session->write_tag_rdma_cmd->sge_list[0].length	=	sizeof(uint32_t);  // Must a atomic write. sizeof(variable) also works
	rdma_session->write_tag_rdma_cmd->sge_list[0].lkey		=	rdma_session->qp->device->local_dma_lkey;
	
	rdma_session->write_tag_rdma_cmd->rdma_sq_wr.wr.sg_list		= rdma_session->write_tag_rdma_cmd->sge_list;  // let wr.sg_list points to the start of the ib_sge array?
	rdma_session->write_tag_rdma_cmd->rdma_sq_wr.wr.num_sge		= 1;											 // 1 RDMA buffer for write_tag


	// 3) Remote RDMA buffers

	// These 2 fields need to be assinged dynamically.
	//rdma_session->write_tag_rdma_cmd->rdma_sq_wr.rkey					= remote_chunk_ptr->remote_rkey;
	//rdma_session->write_tag_rdma_cmd->rdma_sq_wr.remote_addr	= remote_chunk_ptr->remote_addr + offset_within_chunk;

	// The rest fields are assigned statically.
	rdma_session->write_tag_rdma_cmd->rdma_sq_wr.wr.opcode			= IB_WR_RDMA_WRITE;  			//
	rdma_session->write_tag_rdma_cmd->rdma_sq_wr.wr.send_flags 	= IB_SEND_SIGNALED; 	//get a ack when 1-sided RDMA message is sent
	rdma_session->write_tag_rdma_cmd->rdma_sq_wr.wr.wr_id	= (uint64_t)rdma_session->write_tag_rdma_cmd;

	//Debug
	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		rdma_session->write_tag_rdma_cmd->message_type	= 2;  // write_tag
		printk(KERN_INFO "%s, initialize the write_tag and write_tag_rdma_command is done. \n",__func__);
	#endif
*/

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
err:
#endif

	return ret;
}
















//
// <<<<<<<<<<<<<<  End of ONE-SIDED RDMA message section <<<<<<<<<<<<<<
//




//
// >>>>>>>>>>>>>>>  Start of handling chunk management >>>>>>>>>>>>>>>
//
// After building the RDMA connection, build the Client-Chunk Remote-Chunk mapping information.
//



/**
 * Invoke this information after getting the free size of remote memory pool.
 * Initialize the chunk_list based on the chunk size and remote free memory size.
 * 
 *  
 * More Explanation:
 * 	Record the address of the mapped chunk:
 * 		remote_rkey : Used by the client, read/write data here.
 * 		remote_addr : The actual virtual address of the mapped chunk ?
 * 
 */
int init_remote_chunk_list(struct rdma_session_context *rdma_session ){

	int ret = 0;
	uint32_t i;
	
	// 1) initialize chunk related variables
	//		The first Region may not be fullly mapped. Not clear for now.
	//		The 2nd -> rest are fully mapped at REGION_SIZE_GB size.
	rdma_session->remote_chunk_list.chunk_ptr = 0;	// Points to the first empty chunk.
	rdma_session->remote_chunk_list.remote_free_size = 0; // not clear the exactly free size now.
	rdma_session->remote_chunk_list.remote_chunk = (struct remote_mapping_chunk*)kzalloc(  \
																									sizeof(struct remote_mapping_chunk) * rdma_session->remote_chunk_list.chunk_num,\
																									GFP_KERNEL);

	for(i=0; i < rdma_session->remote_chunk_list.chunk_num; i++){
		rdma_session->remote_chunk_list.remote_chunk[i].chunk_state = EMPTY;
		rdma_session->remote_chunk_list.remote_chunk[i].remote_addr = 0x0;
		rdma_session->remote_chunk_list.remote_chunk[i].mapped_size	= 0x0;
		rdma_session->remote_chunk_list.remote_chunk[i].remote_rkey = 0x0;
	}



	return ret;
}




/**
 * Get a chunk mapping 2-sided RDMA message.
 * Bind these chunks to the cient in order.
 * 
 *	1) The information of Chunks to be bound,  is stored in the recv WR associated DMA buffer.
 * Record the address of the mapped chunk:
 * 		remote_rkey : Used by the client, read/write data here.
 * 		remote_addr : The actual virtual address of the mapped chunk
 * 
 *	2) Attach the received chunks to the rdma_session_context->remote_mapping_chunk_list->remote_mapping_chunk[]
 */
void bind_remote_memory_chunks(struct rdma_session_context *rdma_session ){

	int i; 
	uint32_t *chunk_ptr;

	chunk_ptr = &(rdma_session->remote_chunk_list.chunk_ptr);
	// Traverse the receive WR to find all the got chunks.
	for(i = 0; i < MAX_REGION_NUM; i++ ){
		
		#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		if( *chunk_ptr >= rdma_session->remote_chunk_list.chunk_num){
			printk(KERN_ERR "%s, Get too many chunks. \n", __func__);
			break;
		}
		#endif
		
		if(rdma_session->rdma_recv_req.recv_buf->rkey[i]){
			// Sent chunk, attach to current chunk_list's tail.
			rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].remote_rkey = rdma_session->rdma_recv_req.recv_buf->rkey[i];
			rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].remote_addr = rdma_session->rdma_recv_req.recv_buf->buf[i];
			rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].mapped_size = rdma_session->rdma_recv_req.recv_buf->mapped_size[i];
			rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].chunk_state = MAPPED;
			
			rdma_session->remote_chunk_list.remote_free_size += rdma_session->rdma_recv_req.recv_buf->mapped_size[i]; // byte size, 4KB alignment


			#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
				printk(KERN_INFO "Got chunk[%d] : remote_addr : 0x%llx, remote_rkey: 0x%x, mapped_size: 0x%llx \n", i, 
																						rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].remote_addr,
																						rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].remote_rkey,
																						rdma_session->remote_chunk_list.remote_chunk[*chunk_ptr].mapped_size);
			#endif

			(*chunk_ptr)++;
		}

	} // for

	rdma_session->remote_chunk_list.chunk_num	= *chunk_ptr;  // Record the number of received chunks.

}





//
// <<<<<<<<<<<<<<<<<<<<<  End of handling chunk management <<<<<<<<<<<<<<<<<<<<<
//




//
// >>>>>>>>>>>>>>>  Start of  RDMA Control Path >>>>>>>>>>>>>>>
//





/**
 * Semeru CS - Copy the received data from RDMA buffer to destination.
 *  
 * 1) For current design, kernel register the passed in user space as RDMA buffer directly to acheive zero copy.
 * 		a. The caller progress can't quit before the value.
 *    b. The caller progress has to pin the physical memory to not let kernel unmap the physical page from it.
 * 
 * 2）Inform casller progress that the feteched data comes back.
 * 		a. Both caller and responser need to negotiate a specific uint64_t as flag.
 * 		b. Caller reset it to a specific vlaue, e.g. -1 for uint64_t, before send 1-sided RDMA read.
 *    c. The flag value on responser are initiazed to a tag value, e.g. server#1.
 *    d. Caller use a busy waiting loop to check the flag value.
 */
void cp_rdma_read_done(struct ib_cq *cq, struct ib_wc *wc){
	int ret = 0;
  struct semeru_rdma_req_sg 	*rdma_cmd_ptr;
	struct semeru_rdma_queue	*rdma_queue;
  
	// Get rdma_command  attached to wr->wr_id
	// Reuse the rmem_rdam_command instance.
  rdma_cmd_ptr	= container_of(wc->wr_cqe, struct semeru_rdma_req_sg, cqe); 
  if(unlikely(rdma_cmd_ptr == NULL)){
    printk(KERN_ERR "%s, get NULL rmem_rdma_command from wc \n", __func__);
		ret = -1;
		goto out;
  }

	rdma_queue = rdma_cmd_ptr->rdma_queue;
	
	// unmap rdma buffer from device
	// [?] if we keep this mapping ,will it batter for our re-map next time ?
	ib_dma_unmap_sg(rdma_queue->rdma_session->rdma_dev->dev, rdma_cmd_ptr->sgl, rdma_cmd_ptr->nentry,	DMA_FROM_DEVICE);

	// Return one wr, decrease the number of outstanding (read) wr.
	ret = atomic_dec_return(&(rdma_queue->rdma_post_counter));
	complete(&rdma_cmd_ptr->done); 

	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk(KERN_INFO "%s, rdma_queue[%d], rdma_wr[%d] done. <<<< \n", __func__, rdma_queue->q_index, ret + 1);
	#endif

out:
	return;
}


/**
 * Almost same with read done. 
 * 
 */
void cp_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc){
	int ret = 0;
  	struct semeru_rdma_req_sg 	*rdma_cmd_ptr;
	struct semeru_rdma_queue	*rdma_queue;
  
	// Get rdma_command  attached to wr->wr_id
	// Reuse the rmem_rdam_command instance.
  	rdma_cmd_ptr	= container_of(wc->wr_cqe, struct semeru_rdma_req_sg, cqe); 
  	if(unlikely(rdma_cmd_ptr == NULL)){
    		printk(KERN_ERR "%s, get NULL rmem_rdma_command from wc \n", __func__);
		ret = -1;
		goto out;
  	}

	rdma_queue = rdma_cmd_ptr->rdma_queue;
	

	// unmap rdma buffer from device
	// [?] if we keep this mapping ,will it batter for our re-map next time ?
	ib_dma_unmap_sg(rdma_queue->rdma_session->rdma_dev->dev, rdma_cmd_ptr->sgl, rdma_cmd_ptr->nentry,	DMA_TO_DEVICE);

	// Return one wr, decrease the number of outstanding (read) wr.
	ret = atomic_dec_return(&(rdma_queue->rdma_post_counter));
	complete(&rdma_cmd_ptr->done); 

	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk(KERN_INFO "%s, rdma_queue[%d], rdma_wr[%d] done. <<<< \n", __func__, rdma_queue->q_index, ret+1);
	#endif

out:
	return;
}

/**
 * When we enqueue a write/read wr,
 * the total number can't exceed the send/receive queue depth.
 * Or it will cause QP Out of Memory error.
 * 
 * return :
 *  0 : success;
 *  -1 : error. 
 * 
 * More explanation:
 * There are 2 queues for QP, send/recv queue.
 * 1) Send queue limit the number of outstanding wr.
 *    This limits both the 1-sided/2-sided wr. 
 * 2) For 2-sided RDMA, it also needs to post s recv wr to receive data.
 *    But for Semeru, we reply on 1-sided RDMA wr to write/read data.
 *    The the depth of send queue should be much larger than recv queue.
 * 
 * The depth of these 2 queues are limited by:
 * 	init_attr.cap.max_send_wr
 *	init_attr.cap.max_recv_wr
 * 
 */
int cp_enqueue_send_wr(struct rdma_session_context *rdma_session, struct semeru_rdma_queue *rdma_queue,
		       struct semeru_rdma_req_sg *rdma_req)
{
	int ret = 0;
	struct ib_send_wr *bad_wr;
	int test;

	rdma_req->rdma_queue = rdma_queue; // points to the rdma_queue to be enqueued.

	// Post 1-sided RDMA read wr
	// wait and enqueue wr
	// Both 1-sided read/write queue depth are RDMA_SEND_QUEUE_DEPTH
	while (1) {
		test = atomic_inc_return(&rdma_queue->rdma_post_counter);
		if (test < RDMA_SEND_QUEUE_DEPTH - 16) {
			//post the 1-sided RDMA write
			// Use the global RDMA context, rdma_session_global
			ret = ib_post_send(rdma_queue->qp, (struct ib_send_wr *)&rdma_req->rdma_sq_wr, &bad_wr);
			if (unlikely(ret)) {
				printk(KERN_ERR "%s, post 1-sided RDMA send wr failed, return value :%d. counter %d \n",
				       __func__, ret, test);
				ret = -1;
				goto err;
			}

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
			printk(KERN_INFO "%s, rdma_queue[%d] enqueued rdma_wr[%d] >>>> \n", __func__,
			       rdma_queue->q_index, test);
#endif
			// Enqueue successfully.
			// exit loop.
			return ret;
		} else {
			// RDMA send queue is full, wait for next turn.
			test = atomic_dec_return(&rdma_queue->rdma_post_counter);
			//schedule(); // release the core for a while.
			// cpu_relax(); // which one is better ?

			// For IB_DIRCT_CQ, poll the cq
			drain_rdma_queue(rdma_queue);
		}

	} // end of while, try to enqueue read wr.

err:
	printk(KERN_ERR " Error in %s \n", __func__);
	return -1;
}

/**
 * Semeru CS - Map multiple meta data structure's physical address to rdma scatter-gather.
 * 
 * Find several contiguous virtual pages and register them as RDMA buffer for a S/G WR.
 * The max contiguous pages number is (MAX_REQUEST_SGL -2).  2 for safety.
 * 
 * 
 * Build the RDMA buffer of CPU server. 
 * CPU Server : multiple sg entries, their physical/dma address are not contiguous.
 * Memory Server : a contiguous virtual RDMA buffer. 
 * 
 *  Parameters
 * 	*addr_scan_ptr : 
 * 	entry point - points to the start addr to be scanned.
 * 	exit point - points to the end addr of the package page.
 * 
 * 	|- #1 - #2 - #3 - #5 - #6 - .... |
 *                              ^
 * 	*addr_scan_ptr points the end of page #5, start of page #6.
 *       page #5 is the last page in the package. It's mapped.			
 * 
 */
uint64_t meta_data_map_sg(struct rdma_session_context *rdma_session, struct scatterlist *sgl, char **addr_scan_ptr,
			  char *end_addr)
{
	uint64_t entries = 0; // mapped pages
	size_t package_page_num_limit = (MAX_REQUEST_SGL - 2); // InfiniBand hardware S/G limits, bytes
	//size_t package_page_num_limit = 1; // InfiniBand hardware S/G limits, bytes

	pte_t *pte_ptr;
	struct page *buf_page;

	// Scan and find several, at most package_len_limit, contiguous pages as RDMA buffer.
	while (*addr_scan_ptr < end_addr) {
		// [?] Will this walking cause too much overhead ?
		// [?] It's much safer to lock the pmd here. 
		// Or the users need to guarantte there is no swap/user-modification to the page-table.
		pte_ptr = walk_page_table(current->mm, (uint64_t)(*addr_scan_ptr)); 


		// 1) Never touched, skip this page.
		if(pte_ptr == NULL)
			goto skip_page;

		// 2)not present : the page is swapped out, OR it's being swapped out.
		//
		// if the page is dirty and stays in swap_cache, 
		// we have to flush it to remote memory servesr.
		if (!pte_present(*pte_ptr)) {

			// if the page is unmapped, in swap cache and dirty
			// control path need to write it to remote memory server
			// Or the memory server tracing can be wrong.
			//
			// Warning : the try_to_find_page_in_swap_cache will get_page, 
			// which increased the reference count of the page.
			// if the data path is swapping out this page, this can cause race problem.
			// Must drop the page->_refcount via put_page after invoking the page_in_swap_cache()
			//
	

			// flush the swap-cached dirty page to memory servers
		
			buf_page = page_in_swap_cache(*pte_ptr);
			if ( buf_page != NULL  ) {

				if(PageDirty(buf_page) ){
//#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
				 	printk(KERN_WARNING "%s, Virt page 0x%llx -> (dirty) phys page is in Swap Cache. Flushed.\n", __func__,
				        		(uint64_t)(*addr_scan_ptr));
//#endif						

					// drop the refcount by one here.
					put_page(buf_page);
					goto found_page;
				} // find a swap-cached dirty page
					
				// skip thsi page
				// drop the refcount by one
				put_page(buf_page);
			} // target page is in swap-cache

			goto skip_page;
		} // end of non-present

		
		// 3) Default-Path : Page Walk the mapped pte.
		//
		// TO be done: filter out the non-dirty pages. 
		//
		buf_page = pfn_to_page(pte_pfn(*pte_ptr));

found_page:
		// Assign a page to s/g. entire apge, offset in page is 0x0,.
		sg_set_page(&(sgl[entries++]), buf_page, PAGE_SIZE, 0); 

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk(KERN_INFO "%s, Virt page 0x%lx, pte 0x%lx, page 0x%lx, dma_addr 0x%lx \n", __func__,
		       (size_t)*addr_scan_ptr, (size_t)pte_val(*pte_ptr), (size_t)buf_page,
		       (size_t)sgl[entries - 1].dma_address);
#endif

		// Find a mapped page, update the interator pointer
		*addr_scan_ptr += PAGE_SIZE;

		// Exit#1, Find enough contiguous virtual pages.
		if (entries >= package_page_num_limit) {
			goto out;
		}
		
		continue; // goto find the first mapped pte.


skip_page:
		// Exit#2, Find a breaking point.
		// Stop building the S/G buffer.
		if (entries != 0) {
			goto out; // Break RDMA buffer registration
		} else {
			// Skip the unmapped page at the beginning, update the iterator.
			*addr_scan_ptr += PAGE_SIZE;
		}


	} // end of while

out:
	return entries; // number of initialized ib_sge
}




/**
 * Control-Path, build a rdma wr for the RDMA read/write in CP.
 * 
 * Return : The number of pages being sent.
 * 
 * 1) Put the contiguous pages into one RDMA wr by utilizing Scatter/Gather.
 * 		Every time we just put a contiguous range of virtual memory into the S/G buffer.
 * 2) start_addr/end_addr stores the virtual address range to be processed.
 * 
 */
int cp_build_rdma_wr(struct rdma_session_context *rdma_session, struct semeru_rdma_req_sg *rdma_cmd_ptr,
		     enum dma_data_direction dir, struct remote_mapping_chunk *remote_chunk_ptr, char **addr_scan_ptr,
		     char *end_addr)
{
	int mapped_pages = 0;
	int i;
	int dma_entry = 0;
	struct ib_device *ibdev = rdma_session->rdma_dev->dev; // get the ib_devices

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	 printk(KERN_INFO "%s, build wr for range [0x%llx, 0x%llx)\n", __func__, (uint64_t)*addr_scan_ptr,
	        (uint64_t)end_addr);
#endif

	// 1) Register the CPU server's local RDMA buffer.
	//	  Map the corresponding physical pages to S/G structure.
	init_completion(&(rdma_cmd_ptr->done));
	rdma_cmd_ptr->nentry = meta_data_map_sg(rdma_session, rdma_cmd_ptr->sgl, addr_scan_ptr, end_addr);
	rdma_cmd_ptr->seq_type = CONTROL_PATH_MEG; // means this is CP path data.
	if (unlikely(rdma_cmd_ptr->nentry == 0)) {
		// It's ok, the pte are not mapped to any physical pages.
		//printk(KERN_INFO "%s, Find zero mapped pages, end at 0x%lx. Skip this wr. \n", __func__,
		 //      (size_t)end_addr);
		mapped_pages = 0;
		goto err; // return 0.
	}

	// 2) Register the physical address stored in S/G structure to DMA device.
	//  	Here gets the scatterlist->dma_address.
	//    CPU server RDMA buffer  registration.
	// Inform PCI device the dma address of these scatterlist.
	dma_entry = ib_dma_map_sg(ibdev, rdma_cmd_ptr->sgl, rdma_cmd_ptr->nentry, dir);
	if (unlikely(dma_entry != rdma_cmd_ptr->nentry)) {
		printk(KERN_ERR "ERROR in %s Registered %d entries to rdma scatterlist, less than expected %llu \n", 
			__func__, dma_entry, rdma_cmd_ptr->nentry);
		mapped_pages = 0;
		goto err;
	}
	mapped_pages = dma_entry; // return the number of pages mapped to RDMA device.

	// 3) Register Remote RDMA buffer to WR.
	// 		The whole remote virtual memory pool is already resigered as RDMA buffer.
	//		Here just fills the information into the rdma_sq_wr.
	rdma_cmd_ptr->rdma_sq_wr.rkey = remote_chunk_ptr->remote_rkey;
	// Start address of the S/G vector. Universal address space.
	rdma_cmd_ptr->rdma_sq_wr.remote_addr = remote_chunk_ptr->remote_addr +
		((uint64_t)(*addr_scan_ptr - rdma_cmd_ptr->nentry * PAGE_SIZE) & CHUNK_MASK);
	rdma_cmd_ptr->rdma_sq_wr.wr.opcode = (dir == DMA_TO_DEVICE ? IB_WR_RDMA_WRITE : IB_WR_RDMA_READ);
	rdma_cmd_ptr->rdma_sq_wr.wr.send_flags = IB_SEND_SIGNALED; // 1-sided RDMA message ? both read /write

	if (dir == DMA_TO_DEVICE) {
		rdma_cmd_ptr->cqe.done = cp_rdma_write_done;
	} else {
		// DMA_FROM_DEVICE
		rdma_cmd_ptr->cqe.done = cp_rdma_read_done;
	}
	rdma_cmd_ptr->rdma_sq_wr.wr.wr_cqe = &(rdma_cmd_ptr->cqe); // completion function

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	if (rdma_cmd_ptr->nentry != dma_entry) {
		printk(KERN_ERR "%s, rdma_cmd_ptr->nentry 0x%lx NOT equal to dma_entry 0x%x \n", __func__,
		       (size_t)rdma_cmd_ptr->nentry, dma_entry);
		mapped_pages = 0; // for current function 0 is error
		goto err;
	}

	// printk(KERN_INFO
	//        "%s,  Mapped %d entries. From CPU server [0x%lx, 0x%lx) to Memory server[0x%lx, 0x%lx) , in current rdma_wr \n",
	//        __func__, dma_entry, (size_t)(*addr_scan_ptr - dma_entry * PAGE_SIZE), (size_t)*addr_scan_ptr,
	//        (size_t)rdma_cmd_ptr->rdma_sq_wr.remote_addr,
	//        (size_t)(rdma_cmd_ptr->rdma_sq_wr.remote_addr + dma_entry * PAGE_SIZE));

	//print_scatterlist_info(rdma_cmd_ptr->sgl, rdma_cmd_ptr->nentry);
#endif

	// 4) Register Local RDMA Buffer to WR.
	// [Warning] assume all the sectors in this bio is contiguous.
	// Then the remote address is contiguous (virtual space).
	// Scatter-Gather can only be one to multiple/mutiple to one.
	//
	// build multiple ib_sge
	//struct ib_sge sge_list[dma_entry];
	//
	// [?] not use the scatterlist->page_link at all ?
	for (i = 0; i < dma_entry; i++) {
		rdma_cmd_ptr->sge_list[i].addr = sg_dma_address(&(rdma_cmd_ptr->sgl[i])); // scatterlist->addr
		rdma_cmd_ptr->sge_list[i].length =
			PAGE_SIZE; // should be the size for each ib_sge !! not the total size of RDMA S/G !!
		rdma_cmd_ptr->sge_list[i].lkey = rdma_session->rdma_dev->dev->local_dma_lkey;

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		// printk(KERN_INFO "%s, Local RDMA Buffer[%d], ib_sge list addr: 0x%llx, lkey: 0x%llx, len: 0x%llx \n",
		//        __func__, i, (uint64_t)rdma_cmd_ptr->sge_list[i].addr, (uint64_t)rdma_cmd_ptr->sge_list[i].lkey,
		//        (uint64_t)rdma_cmd_ptr->sge_list[i].length);
#endif
	} // end of for

	// [x] If not set it to NULL, IB treats the pointed address as a ib_rdma_wr too.
	rdma_cmd_ptr->rdma_sq_wr.wr.next = NULL; 
	// let wr.sg_list points to the start of the ib_sge array
	rdma_cmd_ptr->rdma_sq_wr.wr.sg_list = rdma_cmd_ptr->sge_list; 
	rdma_cmd_ptr->rdma_sq_wr.wr.num_sge = dma_entry;

err:
	return mapped_pages;
}

/**
 * Semeru CPU Server - Control Path, Post 1-sided rdma_wr 
 * 	Invoked by user space application, e.g. JVM.
 * 
 * Parameters 
 * 	start_addr,  remote virtual memory address, to be read.
 * 	data length , 4KB alignment.
 * 
 * 	[?]For the ideal case, don't use a RDMA buffer. fetch the data back into CS's corresponding virtual address directly.
 * 
 * 
 * More explanation
 * 		
 * 
 */
int semeru_cp_rdma_send(struct rdma_session_context *rdma_session, struct semeru_rdma_queue *rdma_queue,
			struct semeru_rdma_req_sg *rdma_req_sg, char __user *start_addr, uint64_t bytes_len,
			enum dma_data_direction dir)
{
	int ret = 0;
	char *end_addr = start_addr + bytes_len;
	char *addr_scan_ptr = start_addr; // Points to the current scanned addr

	// 1) Calculate the remote address
	//    The caller has to guarantee the accessd range within one rdma chunk.
	// REGION_SIZE_GB/chunk in default.
	uint64_t start_chunk_index = ((uint64_t)start_addr - SEMERU_START_ADDR) >> CHUNK_SHIFT; 
	struct remote_mapping_chunk *remote_chunk_ptr =
		&(rdma_session->remote_chunk_list.remote_chunk[start_chunk_index]);

	// Cut the whole data into several packages, limited by the scatter-gather hardware limitations.
	while (addr_scan_ptr < end_addr) {
		ret = cp_build_rdma_wr(rdma_session, rdma_req_sg, dir, remote_chunk_ptr, &addr_scan_ptr, end_addr);
		if (unlikely(ret == 0)) {
			printk(KERN_WARNING
			       "%s, Build rdma wr in Control-Path failed OR Skip empty pte. Skip enqueue WR \n",
			       __func__);
			complete(&rdma_req_sg->done); // Not enqueue this rdma_queue, mark it complete here.
			// ret = 0 is good here. But can cause error in the caller.
			goto err; // Skip the WR enqueue.
		}

		#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk("%s,Post a 1-sided RDMA wr , dir %d , start addr 0x%llx done.\n", __func__, dir,
		       (uint64_t)(addr_scan_ptr - ret * PAGE_SIZE));
		#endif

		// Post 1-sided RDMA read wr
		// Both read/write queue depth are RDMA_SEND_QUEUE_DEPTH
		ret = cp_enqueue_send_wr(rdma_session, rdma_queue, rdma_req_sg);
		if (unlikely(ret)) { // -1, non-zero
			printk(KERN_ERR "%s, enque ib_send_wr failed. \n", __func__);
			goto err;
		}


		//debug - drain after inserting any rdma_wr
		// drain after queue is full
		//drain_rdma_queue(rdma_queue);

	} // end of for loop, send data.

err:
	return ret;
}

//
// Syscall filling operations
//

/**
 * Semeru CPU Server - Synchronous read. with S/G.
 * 	the kernel space rdma read operation. 
 * 	Register the function into kernel's syscall.
 * 
 * @parameters
 * 	int mem_server_id : user need to specify the memory server id
 * 	char* start_addr : the CPU server virtual address
 * 	unsigned long size : the byte size to send
 */
char *semeru_cp_rdma_read(int mem_server_id, char __user *start_addr, unsigned long size)
{
	int ret = 0;
	int cpu;
	char __user *start_addr_aligned;
	char __user *end_addr_aligned;
	unsigned long size_aligned;
	struct semeru_rdma_queue *rdma_queue;
	struct rdma_session_context *rdma_session = &rdma_session_global_ptr[mem_server_id];
	struct semeru_rdma_req_sg *rdma_req_sg;

//#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
if(((size_t)start_addr>=0x400100000000ULL && (size_t)start_addr <= 0x400108000000) || ((size_t)start_addr>=0x400500000000ULL && (size_t)start_addr <= 0x400508000000))
	printk(KERN_INFO " %s, memory_server[%d] start_addr : 0x%lx, size : 0x%lx \n", 
		__func__, mem_server_id, (unsigned long)start_addr, size);
//#endif

	// #1 Do page alignmetn,
	// If the sent data small than a page, align up to a page
	// Because we need to register a whole physical page as RDMA buffer.
	start_addr_aligned = (char *)((unsigned long)start_addr & PAGE_MASK); // align_down
	end_addr_aligned = (char *)(((unsigned long)start_addr + size + PAGE_SIZE - 1) & PAGE_MASK); // align_up
	size_aligned = (unsigned long)(end_addr_aligned - start_addr_aligned);

	cpu = get_cpu(); // disable core preempt

	// !! Fix me !!
	// Control path use the reserved QP on core #control_path_fixed_qp
	rdma_queue = &(rdma_session->rdma_queues[control_path_fixed_qp]);
	rdma_req_sg = (struct semeru_rdma_req_sg *)kmem_cache_alloc(rdma_queue->rdma_req_sg_cache, GFP_ATOMIC);
	if (unlikely(rdma_req_sg == NULL)) {
		pr_err("%s, get reserved rdma_req_sg failed. \n", __func__);
		ret = -1;
		goto out;
	}
	//reset_semeru_rdma_req_sg(rdma_req_sg);
	memset(rdma_req_sg, 0, sizeof(struct semeru_rdma_req_sg));

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk(KERN_INFO "%s, rdma_queue[%d],	aligned start_addr : 0x%lx, aligned size : 0x%lx \n", __func__,
	       rdma_queue->q_index, (unsigned long)start_addr_aligned, size_aligned);
#endif

	// 1) build and enqueue the 1-sided rdma_wr
	ret = semeru_cp_rdma_send(rdma_session, rdma_queue, rdma_req_sg, start_addr_aligned, size_aligned,
				  DMA_FROM_DEVICE);
	if (unlikely(ret != 0)) {
		printk(KERN_ERR "%s, cp_post_rdma_read failed. \n", __func__);
		start_addr = NULL;
		goto out;
	}


	// 2) Wait for the completion.
	// There is no physical page.
	// So, we just drain the corresponding queue
	drain_rdma_queue(rdma_queue); // poll the corresponding RDMA CQ

	put_cpu(); // enable core preemtp

	// 5ms at most. The waiting is un-interrupptible
	ret = wait_for_completion_timeout(&(rdma_req_sg->done),  msecs_to_jiffies(5)); 
	if (unlikely(ret == 0)) {
		pr_err("%s, rdma_queue[%d] wait for rdma_req_sg timeout for 5ms.\n", __func__, rdma_queue->q_index);
		ret = -1;
		goto out;
	}
	ret = 0; // reset return value to 0.

out:
	return start_addr;
}

/**
 * Semeru Control Path - Synchronous write
 * Write data to remote memory pool.
 * 
 * Parameters:
 * target_mem_server : which server to send the data.
 * write_type: 0 for data; 1(non-zero) for signal.
 * 	[x] Lots of flag/signals are written by this function.
 * 		We need to guarantee the signal is the last message on the QP.
 * 		For example, when we issue an CSet to Memory server, we need to confirm all the other packages are already done.
 * 
 * 
 * 
 * If succeed, 
 * 		return the start_addr.
 * else
 * 		return NULL.
 * 
 */
char *semeru_cp_rdma_write(int mem_server_id, int write_type, char __user *start_addr, unsigned long size)
{
	int ret = 0;
	int cpu;
	char __user *start_addr_aligned;
	char __user *end_addr_aligned;
	unsigned long size_aligned;
	struct semeru_rdma_queue *rdma_queue;
	struct rdma_session_context *rdma_session = &rdma_session_global_ptr[mem_server_id];
	struct semeru_rdma_req_sg *rdma_req_sg;

//#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
//if((((size_t)start_addr>=0x400100000000ULL && (size_t)start_addr <= 0x400108000000)) || ((size_t)start_addr>=0x400500000000ULL && (size_t)start_addr <= 0x400508000000))
	// printk(KERN_INFO " %s, mem_server[%d] write_type 0x%x, start_addr : 0x%lx, size : 0x%lx \n", 
	// 	__func__, mem_server_id, write_type, (unsigned long)start_addr, size);
//#endif

	// #1 Do page alignment,
	// If the sent data small than a page, align up to a page
	// Because we need to register a whole physical page as RDMA buffer.
	start_addr_aligned = (char *)((unsigned long)start_addr & PAGE_MASK); // align_down
	end_addr_aligned = (char *)(((unsigned long)start_addr + size + PAGE_SIZE - 1) & PAGE_MASK); // align_up
	size_aligned = (unsigned long)(end_addr_aligned - start_addr_aligned);

	cpu = get_cpu(); // disable core preempt

	// !! Fix me !!
	// Control path use the reserved QP on core #control_path_fixed_qp
	rdma_queue = &(rdma_session->rdma_queues[control_path_fixed_qp]);
	rdma_req_sg = (struct semeru_rdma_req_sg *)kmem_cache_alloc(rdma_queue->rdma_req_sg_cache, GFP_ATOMIC);
	if (unlikely(rdma_req_sg == NULL)) {
		pr_err("%s, get reserved rdma_req_sg failed. \n", __func__);
		ret = -1;
		goto out;
	}
	//reset_semeru_rdma_req_sg(rdma_req_sg);
	memset(rdma_req_sg, 0, sizeof(struct semeru_rdma_req_sg));

#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk(KERN_INFO "%s, mem_server[%d] rdma_queue[%d], aligned start_addr : 0x%lx, aligned size : 0x%lx \n", __func__,
	       mem_server_id, rdma_queue->q_index, (unsigned long)start_addr_aligned, size_aligned);
#endif

	// 1) Drain all the outstanding requests for a signal write
	if (write_type) { // no-zero
		drain_all_rdma_queue(mem_server_id);
	}

	// 2) build and enqueue the 1-sided rdma_wr
	// [x] Get the rdma_session_context *rdma_session
	// [?] How to confirm the rdma_session_global is fully initialized ?
	ret = semeru_cp_rdma_send(rdma_session, rdma_queue, rdma_req_sg, start_addr_aligned, size_aligned,
				  DMA_TO_DEVICE);
	if (unlikely(ret != 0)) {
		printk(KERN_ERR "%s, cp_post_rdma_read failed. \n", __func__);
		start_addr = NULL;
		goto out;
	}



	// 1) Wait for the completion.
	// There is no physical page.
	// So, we just drain the corresponding queue
	drain_rdma_queue(rdma_queue); // poll the corresponding RDMA CQ

	put_cpu(); // enable core preemtp

	// 5ms at most. The waiting is un-interrupptible
	ret = wait_for_completion_timeout(&(rdma_req_sg->done), msecs_to_jiffies(5)); 
	if (unlikely(ret == 0)) {
		pr_err("%s, rdma_queue[%d] wait for rdma_req_sg timeout for 5ms.\n", __func__, rdma_queue->q_index);
		ret = -1;
		goto out;
	}
	ret = 0; // reset return value to 0.

out:
	return start_addr;
}



/**
 * Reset all the fields 
 */
void reset_semeru_rdma_req_sg(struct semeru_rdma_req_sg *rmem_rdma_cmd_ptr)
{
	rmem_rdma_cmd_ptr->seq_type = END_TYPE;
	memset(rmem_rdma_cmd_ptr->sge_list, 0,
	       MAX_REQUEST_SGL * sizeof(rmem_rdma_cmd_ptr->sge_list)); // works for stack array

	rmem_rdma_cmd_ptr->nentry = 0;
	memset(rmem_rdma_cmd_ptr->sgl, 0,
	       MAX_REQUEST_SGL * sizeof(struct scatterlist)); // heap array
}

/**
 * Semeru CS - RDMA Control-Path initialization
 *  
 * For CP, the source and destination have the same virtual address.
 * e.g.	
 * 		rdma_read(start_addr#01, len)
 * 		src : start_addr#01, len
 * 		dst : start_addr#01, len
 * 		
 * [?] Calculate the corresponding DMA address dynamically ? 
 * 		 Or calculate the corresponding DMA address for meta space at initialization time and time them. 
 * 		 Then we can use these address to build the wr directly. 
 * 
 */
int init_rdma_control_path(struct rdma_session_context *rdma_session){
	int ret = 0;

	/*
	// 2) Allocate and initiate the rdma wr
	//    [x] the rdma_session->cp_rmem_rdma_read/write_cmd->rdma_sq_wr is reused for all the  control-path.
	//				so, assign the addr and length dynamically.
	//        [?] Will this cause problem ?? e.g. the previous write is not finished, and next write begin using this semeru_rdma_req_sg ??
	//
	//    [x] The Control-Path may transfer some big structure, whose physical pages may not contiguous.
	//				Here still needs scatter-gather.
	rdma_session->cp_rmem_rdma_read_cmd = (struct semeru_rdma_req_sg*)kzalloc( sizeof(struct semeru_rdma_req_sg) +  \
																																MAX_REQUEST_SGL*sizeof(struct scatterlist), GFP_KERNEL);
	if(unlikely(rdma_session->cp_rmem_rdma_read_cmd == NULL)){
		printk(KERN_ERR "%s, allocate  cp_rmem_rdma_read_cmd failed.\n",__func__);
		ret = -1;
		goto out;
	}
	reset_semeru_rdma_req_sg(rdma_session->cp_rmem_rdma_read_cmd);

	rdma_session->cp_rmem_rdma_write_cmd = (struct semeru_rdma_req_sg*)kzalloc( sizeof(struct semeru_rdma_req_sg) +  \
																																MAX_REQUEST_SGL*sizeof(struct scatterlist), GFP_KERNEL);
	if(unlikely(rdma_session->cp_rmem_rdma_write_cmd == NULL)){
		printk(KERN_ERR "%s, allocate  cp_rmem_rdma_write_cmd failed.\n",__func__);
		ret = -1;
		goto out;
	}
	reset_semeru_rdma_req_sg(rdma_session->cp_rmem_rdma_write_cmd);


	
	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk(KERN_INFO "%s, initialize meta space structure done, with rdma_session_context:0x%llx \n",
																											__func__,
																											(uint64_t)rdma_session);

		printk(KERN_INFO "%s, allocate & initialized rdma_session->cp_rmem_rdma_read_cmd: 0x%llx \n", __func__,
																																												(uint64_t)rdma_session->cp_rmem_rdma_read_cmd);

		printk(KERN_INFO "%s, allocate & initialized rdma_session->cp_rmem_rdma_write_cmd: 0x%llx \n", __func__,
																																												(uint64_t)rdma_session->cp_rmem_rdma_write_cmd);
		
	#endif 

*/


//out:
	return ret;
}

/**
 * Register module defined functions into kernel.
 *  
 */
void init_kernel_semeru_rdma_ops(void)
{
#ifndef DEBUG_BD_ONLY // For block device only mode, no need to register the RDMA function
	struct semeru_rdma_ops module_rdma_ops; // temporary var
	// the address of function is fixed.
	module_rdma_ops.rdma_read = &semeru_cp_rdma_read; 
	module_rdma_ops.rdma_write = &semeru_cp_rdma_write;

	rdma_ops_wrapper(&module_rdma_ops); // exported kernel call
#endif

	return;
}


/**
 * Safely removed the registered rdma operations.
 *  
 */
void reset_kernel_semeru_rdma_ops(void)
{
#ifndef DEBUG_BD_ONLY
	struct semeru_rdma_ops module_rdma_ops; // temporary var
	module_rdma_ops.rdma_read = NULL; // reset to NULL
	module_rdma_ops.rdma_write = NULL;

	rdma_ops_wrapper(&module_rdma_ops); // exported kernel call
#endif

	return;
}

//
// <<<<<<<<<<<<<<<<<<<<<  End of RDMA Control Path <<<<<<<<<<<<<<<<<<<<<
//



//
// >>>>>>>>>>>>>>>  Start of RDMA intialization >>>>>>>>>>>>>>>
//

/**
 * @brief the global handler of all the rdma_session_context. 
 *	One rdma_session_context for each memory server.
 * 
 * @param rdma_session_global_ptr 
 * @return int :
 * 	0 : success
 * 	non-zero : error.
 */
int init_rdma_sessions(struct rdma_session_context **rdma_session_global_ptr_addr)
{
	int ret = 0;
	int mem_server_id;
	struct rdma_session_context * rdma_session_ptr = *rdma_session_global_ptr_addr;

	*rdma_session_global_ptr_addr = kzalloc(sizeof(struct rdma_session_context) * NUM_OF_MEMORY_SERVER, GFP_KERNEL);
	if (*rdma_session_global_ptr_addr == NULL) {
		ret = -1;
		pr_err("%s, rdma_session_global allocation failed.", __func__);
		goto out;
	}

	// initialize each rdma_session_context
	rdma_session_ptr = *rdma_session_global_ptr_addr; // get the pointer value
	for (mem_server_id = 0; mem_server_id < NUM_OF_MEMORY_SERVER; mem_server_id++) {
		rdma_session_ptr[mem_server_id].mem_server_id = mem_server_id;
		rdma_session_ptr[mem_server_id].data_region_num = DATA_REGION_PER_MEM_SERVER;
		rdma_session_ptr[mem_server_id].data_region_start_id = RDMA_META_REGION_NUM + mem_server_id * DATA_REGION_PER_MEM_SERVER;

		ret = init_rdma_session(&rdma_session_ptr[mem_server_id]);
		if(ret){
			pr_err("%s, memory_sever[%d], rdma_session init failed", __func__, mem_server_id);
			goto out;
		}
	}

out:
	return ret;
}

/**
 * Init the rdma sessions for a memory server.
 */
int init_rdma_session(struct rdma_session_context *rdma_session )
{
	int ret = 0;
	char *ip;


	ip = mem_server_ip[rdma_session->mem_server_id];

	// 1) RDMA queue information
	// The number of outstanding wr the QP's send queue and recv queue.
	// For Semeru, the on-the-fly RDMA request should be more than on-the-fly i/o request.
	// Semeru use 1-sided RDMA to transfer data.
	// Only 2-sided RDMA needs to post the recv wr.
	rdma_session->rdma_queues = kzalloc(sizeof(struct semeru_rdma_queue) * online_cores, GFP_KERNEL);
	rdma_session->send_queue_depth = RDMA_SEND_QUEUE_DEPTH + 1;
	rdma_session->recv_queue_depth = RDMA_RECV_QUEUE_DEPTH + 1;

	// 2) Setup socket information
	// All the memory servers use the same port, 9400
	rdma_session->port = htons((uint16_t)mem_server_port); // transffer to big endian
	ret = in4_pton(ip, strlen(ip), rdma_session->addr, -1, NULL); // char* to ipv4 address
	if (ret == 0) { // kernel 4.11.0 , success 1; failed 0.
		printk(KERN_ERR "Assign ip %s to  rdma_session->addr : %s failed.\n", ip, rdma_session->addr);
		goto err;
	}
	ret = 0; // reset return value to 0 as success.

	rdma_session->addr_type = AF_INET; //ipv4

err:
	return ret;
}

/**
 * Reserve some rdma_wr, registerred buffer for fast communiction. 
 * 	e.g. 2-sided communication, meta data communication.
 */
int setup_rdma_session_commu_buffer(struct rdma_session_context * rdma_session){

	int ret = 0;

	if(rdma_session->rdma_dev == NULL){
		printk(KERN_ERR "%s, rdma_session->rdma_dev is NULL. too early to regiseter RDMA buffer.\n", __func__);
		goto err;
	}


	// 1) Reserve some buffer for 2-sided RDMA communication
  ret = semeru_setup_buffers(rdma_session);
	if(unlikely(ret)){
		printk(KERN_ERR "%s, Bind DMA buffer error\n", __func__);
		goto err;
	}


	// 2) Control-Path
	ret = init_rdma_control_path(rdma_session);
	if(unlikely(ret)){
		printk(KERN_ERR "%s, initialize 1-sided RDMA buffers error. \n",__func__);
		goto err;
	}

err:
	return ret;
}

/**
 * Build and Connect all the QP for each Session/memory server.
 *  
 */
int semeru_init_rdma_queue(struct rdma_session_context *rdma_session, int cpu)
{
	int ret = 0;
	struct semeru_rdma_queue *rdma_queue = &(rdma_session->rdma_queues[cpu]);

	rdma_queue->rdma_session = rdma_session;
	rdma_queue->q_index = cpu;
	rdma_queue->cm_id = rdma_create_id(&init_net, semeru_rdma_cm_event_handler, rdma_queue, RDMA_PS_TCP, IB_QPT_RC);
	if (IS_ERR(rdma_queue->cm_id)) {
		printk(KERN_ERR "failed to create cm id: %ld\n", PTR_ERR(rdma_queue->cm_id));
		ret = -ENODEV;
		goto err;
	}

	rdma_queue->state = IDLE;
	init_waitqueue_head(&rdma_queue->sem); // Initialize the semaphore.
	spin_lock_init(&(rdma_queue->cq_lock)); // initialize spin lock
	atomic_set(&(rdma_queue->rdma_post_counter), 0); // Initialize the counter to 0
	rdma_queue->fs_rdma_req_cache = kmem_cache_create("fs_rdma_req_cache", sizeof(struct fs_rdma_req), 0,
							  SLAB_TEMPORARY | SLAB_HWCACHE_ALIGN, NULL);
	if (unlikely(rdma_queue->fs_rdma_req_cache == NULL)) {
		printk(KERN_ERR "%s, allocate rdma_queue->fs_rdma_req_cache failed.\n", __func__);
		ret = -1;
		goto err;
	}

	rdma_queue->rdma_req_sg_cache = kmem_cache_create("rdma_req_sg_cache", sizeof(struct semeru_rdma_req_sg), 0,
							  SLAB_TEMPORARY | SLAB_HWCACHE_ALIGN, NULL);
	if (unlikely(rdma_queue->rdma_req_sg_cache == NULL)) {
		printk(KERN_ERR "%s, allocate rdma_queue->rdma_req_sg_cache failed.\n", __func__);
		ret = -1;
		goto err;
	}

	//2) Resolve address(ip:port) and route to destination IB.
	ret = rdma_resolve_ip_to_ib_device(rdma_session, rdma_queue);
	if (unlikely(ret)) {
		printk(KERN_ERR "%s, bind socket error (addr or route resolve error)\n", __func__);
		return ret;
	}

	return ret;

err:
	rdma_destroy_id(rdma_queue->cm_id);
	return ret;
}


/**
 * @brief Connect to each memory servers
 * 
 * @param rdma_session_global_ptr 
 * @return int , 0 for success. non-zero for errors
 */
int rdma_sessions_connect(struct rdma_session_context *rdma_session_global_ptr)
{
	int ret = 0;
	int mem_server_id, queue_index;
	struct rdma_session_context * rdma_session_ptr;

	for(mem_server_id = 0; mem_server_id < NUM_OF_MEMORY_SERVER; mem_server_id++){
		rdma_session_ptr = &rdma_session_global_ptr[mem_server_id];
		ret = rdma_session_connect(rdma_session_ptr);
		if(unlikely(ret)){
			pr_err("%s, conenct to memory sever[%d] failed.", __func__, mem_server_id);
			// disconenct the alrady connected resource.
			for(queue_index = 0; queue_index < online_cores; queue_index++){
				 // Assuming this mem server is crashed.
				rdma_session_ptr->rdma_queues[queue_index].freed = 255;
			}

			// disconenct the connection to other memory servers.
			semeru_disconnect_mem_servers(rdma_session_global_ptr); // disconnect all mem servers.
			goto out;
		}
	}

out:
	return ret;
}

/**
 * Build the RDMA connection to remote memory server.
 * 	
 * Parameters
 * 		rdma_session, RDMA controller/context.
 * 			
 * More Exlanation:
 * 
 */
int rdma_session_connect(struct rdma_session_context *rdma_session)
{
	int ret;
	int i;

	// 1) Build and connect all the QPs of this session
	//
	for (i = 0; i < online_cores; i++) {
		// crete cm_id and other fields e.g. ip
		ret = semeru_init_rdma_queue(rdma_session, i);
		if (unlikely(ret)) {
			printk(KERN_ERR "%s,init rdma queue [%d] failed.\n", __func__, i);
		}

		// Create device PD, QP CP
		ret = semeru_create_rdma_queue(rdma_session, i);
		if (unlikely(ret)) {
			printk(KERN_ERR "%s, Create rdma queues failed. \n", __func__);
		}

		// Connect to memory server
		ret = semeru_connect_remote_memory_server(rdma_session, i);
		if (ret) {
			printk(KERN_ERR "%s: Connect to remote server error \n", __func__);
			goto err;
		}

		pr_warn("%s, RDMA queue[%d] Connectted to remote server[%d] successfully \n", 
			__func__, i, rdma_session->mem_server_id);
	}

	//
	// 2) Get the memory pool from memory server.
	//

	// 2.1 Send a request to query available memory for this rdma_session
	//     All the QPs within one session/memory server share the same avaialble buffer
	ret = semeru_query_available_memory(rdma_session);
	if (unlikely(ret)) {
		printk("%s, request for chunk failed.\n", __func__);
		goto err;
	}

	// 2.2  Request free memory from Semeru Memory Server
	// 1st Region is the Meta Region,
	// Next are serveral Data Regions.
	ret = semeru_requset_for_chunk(rdma_session, rdma_session->remote_chunk_list.chunk_num);
	if (unlikely(ret)) {
		printk("%s, request for chunk failed.\n", __func__);
		goto err;
	}

	// FINISHED.

	// [!!] Only reach here afeter got STOP_ACK signal from remote memory server.
	// Sequence controll - FINISH.
	// Be carefull, this will lead to all the local variables collected.

	// [?] Can we wait here with function : semeru_disconenct_and_collect_resource together?
	//  NO ! https://stackoverflow.com/questions/16163932/multiple-threads-can-wait-on-a-semaphore-at-same-time
	// Use  differeent semapore signal.
	//wait_event_interruptible(rdma_session->sem, rdma_session->state == CM_DISCONNECT);

	//wait_event_interruptible( rdma_session->sem, rdma_session->state == TEST_DONE );

	printk("%s,Exit the main() function with built RDMA conenction rdma_session_context: 0x%llx .\n", __func__,
	       (uint64_t)rdma_session);

	return ret;

err:
	printk(KERN_ERR "ERROR in %s \n", __func__);
	return ret;
}

/**
 * >>>>>>>>>>>>>>> Start of Resource Free Functions >>>>>>>>>>>>>>>
 * 
 * For kernel space, there is no concept of multi-processes. 
 * There is only multilple kernel threads which share the same kernel virtual memory address.
 * So it's the thread's resposibility to free the allocated memory in the kernel heap.
 * 
 * 
 * 
 */



/**
 * Free the RDMA buffers.
 * 
 * 	1) 2-sided DMA buffer
 * 	2) mapped remote chunks.
 * 
 */
void semeru_free_buffers(struct rdma_session_context *rdma_session) {

	// Free the DMA buffer for 2-sided RDMA messages
	if(rdma_session == NULL)
		return;

	// Free 1-sided dma buffers
	if(rdma_session->rdma_recv_req.recv_buf != NULL)
		kfree(rdma_session->rdma_recv_req.recv_buf);
	if(rdma_session->rdma_send_req.send_buf != NULL)
		kfree(rdma_session->rdma_send_req.send_buf);



	// Free the remote chunk management,
	if(rdma_session->remote_chunk_list.remote_chunk != NULL)
		kfree(rdma_session->remote_chunk_list.remote_chunk);

	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk("%s, Free RDMA buffers done. \n",__func__);
	#endif

}


/**
 * Free InfiniBand related structures.
 * 
 * rdma_cm_id : the main structure to maintain the IB.
 * 		
 * 
 * 
 */
void semeru_free_rdma_structure(struct rdma_session_context *rdma_session){

	struct semeru_rdma_queue * rdma_queue;
	int i;

	if (rdma_session == NULL)
		return;

	// Free each QP
	for(i=0; i<online_cores; i++){

		rdma_queue = &(rdma_session->rdma_queues[i]);

		if(rdma_queue->cm_id != NULL){
			rdma_destroy_id(rdma_queue->cm_id);

			#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
			printk("%s, free rdma_queue[%d] rdma_cm_id done. \n",__func__, i);
			#endif
		}

		if(rdma_queue->qp != NULL){
			ib_destroy_qp(rdma_queue->qp);

			#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
			printk("%s, free rdma_queue[%d] ib_qp  done. \n",__func__, i);
			#endif
		}

		// 
		// Both send_cq/recb_cq should be freed in ib_destroy_qp() ?
		//
		if(rdma_queue->cq != NULL){
		ib_destroy_cq(rdma_queue->cq);

			#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
			printk("%s, free rdma_queue[%d] ib_cq  done. \n",__func__, i);
			#endif
		}

	}// end of free RDMA QP

	// Before invoke this function, free all the resource binded to pd.
	if(rdma_session->rdma_dev->pd != NULL){
		ib_dealloc_pd(rdma_session->rdma_dev->pd); 

		#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
		printk("%s, Free device PD  done. \n",__func__);
		#endif
	}

	#if defined(DEBUG_MODE_BRIEF) || defined(DEBUG_MODE_DETAIL)
	printk("%s, Free RDMA structures,cm_id,qp,cq,pd done. \n",__func__);
	#endif

}


/**
 * @brief Disconenct the connection with each memory servers.
 * 
 * @param rdma_session 
 * @return int
 * 	0 : success
 * 	non-zero, errors
 */
int semeru_disconnect_mem_servers(struct rdma_session_context *rdma_session_global)
{
	int ret = 0;
	int mem_server_id;
	struct rdma_session_context *rdma_session_ptr;

	for(mem_server_id = 0; mem_server_id < NUM_OF_MEMORY_SERVER; mem_server_id++){
		rdma_session_ptr = &rdma_session_global[mem_server_id];
		ret = semeru_disconenct_and_collect_resource(rdma_session_ptr);
		if(unlikely(ret)){
			pr_err("%s, errors happened in disconnecting to memory server[%d]",
				__func__, mem_server_id);
			goto out;
		}

		pr_warn("%s, RDMA connection to memory server[%d] disconnectted.", __func__, mem_server_id);
	}


out :
	return ret;
}


/**
 * The main entry of resource free.
 * 
 * [x] 2 call site.
 * 		1) Called by client, at the end of function, octopus_rdma_client_cleanup_module().
 * 		2) Triggered by DISCONNECT CM event, in semeru_rdma_cm_event_handler()
 * 
 */
int semeru_disconenct_and_collect_resource(struct rdma_session_context *rdma_session){

	int ret = 0;
	int i;
	struct semeru_rdma_queue * rdma_queue;
	int mem_server_id = rdma_session->mem_server_id;

	// 1) Disconnect each QP
	//    Issue the diconnection request concurrently
	for(i=0; i< online_cores; i++){
		rdma_queue = &(rdma_session->rdma_queues[i]);

		if(unlikely(rdma_queue->freed != 0)){
			// already called by some thread,
			// just return and wait.
			printk(KERN_WARNING "%s, rdma_queue[%d] already disconnected, no need to send disconnet request. \n"
					,__func__, i);
			continue;
		}

		// Set free state before issue disconnect signal. 
		// This means clien proactively request for disconnection.
		rdma_queue->freed++; 

		// Send disconnection request to memory server.
		if(rdma_queue->state != CM_DISCONNECT){
			ret = rdma_disconnect(rdma_queue->cm_id);
			if(ret){
				printk(KERN_ERR "%s, RDMA disconnect failed. \n",__func__);
				goto err;
			}

			// synchronously disconnection - disconnect each QP one by one.
			// wait the ack of RDMA disconnected successfully
			//wait_event_interruptible(rdma_queue->sem, rdma_queue->state == CM_DISCONNECT); 
		}

		printk("%s, RDMA queue[%d], disconnection request sent to memory server => \n", __func__, i);
	}


	// 2) wait for the receiving all the disconnection signals
	for(i=0; i< online_cores; i++){
		rdma_queue = &(rdma_session->rdma_queues[i]);

		if(rdma_queue->freed == 255){
			pr_warn("%s, rdma_queue[%d] one of memory servers crashed, skip wait on lock.", __func__, i);
			continue;
		}

		wait_event_interruptible(rdma_queue->sem, rdma_queue->state == CM_DISCONNECT); 
		pr_warn("%s, RDMA queue[%d] disconnected. \n", __func__, i);
	}

	pr_warn("%s, All %d RDMA queue disconnected, start resources freeing. \n", __func__, online_cores);


	// 2) Free resouces
	semeru_free_buffers(rdma_session);
	semeru_free_rdma_structure(rdma_session);

	// If not allocated by kzalloc, no need to free it.
	//kfree(rdma_session);  // Free the RDMA context.
	

	// DEBUG -- 
	// Exit the main function first.
	// wake_up_interruptible will cause context switch, 
	// Just skip the code below this invocation.
	//rdma_session_global.state = TEST_DONE;
	//wake_up_interruptible(&(rdma_session_global.sem));


	printk("%s, RDMA memory resouces reserved for memory server[%d] are freed. \n", 
		__func__, mem_server_id);

err:
	return ret;
}


/**
 * <<<<<<<<<<<<<<<<<<<<< End of  Resource Free Functions <<<<<<<<<<<<<<<<<<<<<
 */




//
// Kernel Module registration functions.
//



// invoked by insmod 
int  semeru_fs_rdma_client_init(void){

	int ret = 0;
	printk(KERN_INFO "%s, start \n",__func__);

	// Initialize the RDMA control path, provided by the RDMA driver.
	init_kernel_semeru_rdma_ops();

	// online cores decide the parallelism. e.g. number of QP, CP etc.
	online_cores = num_online_cpus();
	printk(KERN_INFO "%s, online_cores : %d (Can't exceed the slots on Memory server) \n", __func__, online_cores);

	// init the rdma session to memory server
	ret = init_rdma_sessions(&rdma_session_global_ptr);
	BUG_ON(rdma_session_global_ptr == NULL);

	// Build both the RDMA and Disk driver
	ret = rdma_sessions_connect(rdma_session_global_ptr);
	if(unlikely(ret)){
		printk(KERN_ERR "%s, rdma_session_connect failed. \n", __func__);
		goto out;
	}


	// Enable the frontswap path
	ret = semeru_init_frontswap();
	if(unlikely(ret)){
		printk(KERN_ERR "%s, Enable frontswap path failed. \n", __func__);
		goto out;
	}


#ifdef RDMA_MESSAGE_PROFILING
	reset_rdma_message_info();
#endif

out:
	return ret;
}


// invoked by rmmod 
void  semeru_fs_rdma_client_exit(void){
  
	int ret = 0;
	
	// 1) rest control path
	reset_kernel_semeru_rdma_ops();

	// 2) disconect rdma connction
	ret = semeru_disconnect_mem_servers(rdma_session_global_ptr);
	if(unlikely(ret)){
		printk(KERN_ERR "%s,  failed.\n",  __func__);
	}


	// 3) disconnect fontswap path
	semeru_exit_frontswap();

	printk(KERN_INFO "%s done.\n",__func__);

	return;
}






/**
 * >>>>>>>>>>>>>>> Start of Debug functions >>>>>>>>>>>>>>>
 *
 */


//
// Print the RDMA Communication Message 
//
char* rdma_cm_message_print(int cm_message_id){
	char* message_type_name = (char*)kzalloc(32, GFP_KERNEL); // 32 bytes

	switch(cm_message_id){
		case 0:
			strcpy(message_type_name,  "RDMA_CM_EVENT_ADDR_RESOLVED");
			break;
		case 1:
			strcpy(message_type_name, "RDMA_CM_EVENT_ADDR_ERROR");
			break;
		case 2:
			strcpy(message_type_name, "RDMA_CM_EVENT_ROUTE_RESOLVED");
			break;
		case 3:
			strcpy(message_type_name, "RDMA_CM_EVENT_ROUTE_ERROR");
			break;
		case 4:
			strcpy(message_type_name, "RDMA_CM_EVENT_CONNECT_REQUEST");
			break;
		case 5:
			strcpy(message_type_name, "RDMA_CM_EVENT_CONNECT_RESPONSE");
			break;
		case 6:
			strcpy(message_type_name, "RDMA_CM_EVENT_CONNECT_ERROR");
			break;
		case 7:
			strcpy(message_type_name, "RDMA_CM_EVENT_UNREACHABLE");
			break;
		case 8:
			strcpy(message_type_name, "RDMA_CM_EVENT_REJECTED");
			break;
		case 9:
			strcpy(message_type_name, "RDMA_CM_EVENT_ESTABLISHED");
			break;
		case 10:
			strcpy(message_type_name, "RDMA_CM_EVENT_DISCONNECTED");
			break;
		case 11:
			strcpy(message_type_name, "RDMA_CM_EVENT_DEVICE_REMOVAL");
			break;
		case 12:
			strcpy(message_type_name, "RDMA_CM_EVENT_MULTICAST_JOIN");
			break;
		case 13:
			strcpy(message_type_name, "RDMA_CM_EVENT_MULTICAST_ERROR");
			break;
		case 14:
			strcpy(message_type_name, "RDMA_CM_EVENT_ADDR_CHANGE");
			break;
		case 15:
			strcpy(message_type_name, "RDMA_CM_EVENT_TIMEWAIT_EXIT");
			break;
		default:
			strcpy(message_type_name, "ERROR Message Type");
			break;
	}

	return message_type_name;
}


/**
 * wc.status name
 * 
 */
char* rdma_wc_status_name(int wc_status_id){
	char* message_type_name = (char*)kzalloc(32, GFP_KERNEL); // 32 bytes

	switch(wc_status_id){
		case 0:
			strcpy(message_type_name, "IB_WC_SUCCESS");
			break;
		case 1:
			strcpy(message_type_name, "IB_WC_LOC_LEN_ERR");
			break;
		case 2:
			strcpy(message_type_name, "IB_WC_LOC_QP_OP_ERR");
			break;
		case 3:
			strcpy(message_type_name, "IB_WC_LOC_EEC_OP_ERR");
			break;
		case 4:
			strcpy(message_type_name, "IB_WC_LOC_PROT_ERR");
			break;
		case 5:
			strcpy(message_type_name, "IB_WC_WR_FLUSH_ERR");
			break;
		case 6:
			strcpy(message_type_name, "IB_WC_MW_BIND_ERR");
			break;
		case 7:
			strcpy(message_type_name, "IB_WC_BAD_RESP_ERR");
			break;
		case 8:
			strcpy(message_type_name, "IB_WC_LOC_ACCESS_ERR");
			break;
		case 9:
			strcpy(message_type_name, "IB_WC_REM_INV_REQ_ERR");
			break;
		case 10:
			strcpy(message_type_name, "IB_WC_REM_ACCESS_ERR");
			break;
		case 11:
			strcpy(message_type_name, "IB_WC_REM_OP_ERR");
			break;
		case 12:
			strcpy(message_type_name, "IB_WC_RETRY_EXC_ERR");
			break;
		case 13:
			strcpy(message_type_name, "IB_WC_RNR_RETRY_EXC_ERR");
			break;
		case 14:
			strcpy(message_type_name, "IB_WC_LOC_RDD_VIOL_ERR");
			break;
		case 15:
			strcpy(message_type_name, "IB_WC_REM_INV_RD_REQ_ERR");
			break;
		case 16:
			strcpy(message_type_name, "IB_WC_REM_ABORT_ERR");
			break;
		case 17:
			strcpy(message_type_name, "IB_WC_INV_EECN_ERR");
			break;
		case 18:
			strcpy(message_type_name, "IB_WC_INV_EEC_STATE_ERR");
			break;
		case 19:
			strcpy(message_type_name, "IB_WC_FATAL_ERR");
			break;
		case 20:
			strcpy(message_type_name, "IB_WC_RESP_TIMEOUT_ERR");
			break;
		case 21:
			strcpy(message_type_name, "IB_WC_GENERAL_ERR");
			break;
		default:
			strcpy(message_type_name, "ERROR Message Type");
			break;
	}

	return message_type_name;
}



/**
 * The message type name, used for 2-sided RDMA communication.
 */
char* rdma_message_print(int message_id){
	char* message_type_name;
	message_type_name = (char*)kzalloc(32, GFP_KERNEL); // 32 bytes

	switch(message_id){

		case 1:
			strcpy(message_type_name, "DONE");
			break;

		case 2:
			strcpy(message_type_name, "GOT_CHUNKS");
			break;

		case 3:
			strcpy(message_type_name, "GOT_SINGLE_CHUNK");
			break;

		case 4:
			strcpy(message_type_name, "FREE_SIZE");
			break;

		case 5:
			strcpy(message_type_name, "EVICT");
			break;

		case 6:
			strcpy(message_type_name, "ACTIVITY");
			break;

		case 7:
			strcpy(message_type_name, "STOP");
			break;

		case 8:
			strcpy(message_type_name, "REQUEST_CHUNKS");
			break;

		case 9:
			strcpy(message_type_name, "REQUEST_SINGLE_CHUNK");
			break;

		case 10:
			strcpy(message_type_name, "QUERY");
			break;

		case 11 :
			strcpy(message_type_name, "AVAILABLE_TO_QUERY");
			break;

		default:
			strcpy(message_type_name, "ERROR Message Type");
			break;
	}

	return message_type_name;
}



// Print the string of rdma_session_context state.
// void rdma_session_context_state_print(int id){
char* rdma_session_context_state_print(int id){

	char* rdma_seesion_state_name;
	rdma_seesion_state_name = (char*)kzalloc(32, GFP_KERNEL); // 32 bytes.

	switch (id){

		case 1 :
			strcpy(rdma_seesion_state_name, "IDLE");
			break;
		case 2 :
			strcpy(rdma_seesion_state_name, "CONNECT_REQUEST");
			break;
		case 3 :
			strcpy(rdma_seesion_state_name, "ADDR_RESOLVED");
			break;
		case 4 :
			strcpy(rdma_seesion_state_name, "ROUTE_RESOLVED");
			break;
		case 5 :
			strcpy(rdma_seesion_state_name, "CONNECTED");
			break;
		case 6 :
			strcpy(rdma_seesion_state_name, "FREE_MEM_RECV");
			break;
		case 7 :
			strcpy(rdma_seesion_state_name, "RECEIVED_CHUNKS");
			break;
		case 8 :
			strcpy(rdma_seesion_state_name, "RDMA_BUF_ADV");
			break;
		case 9 :
			strcpy(rdma_seesion_state_name, "WAIT_OPS");
			break;
		case 10 :
			strcpy(rdma_seesion_state_name, "RECV_STOP");
			break;
		case 11 :
			strcpy(rdma_seesion_state_name, "RECV_EVICT");
			break;
		case 12 :
			strcpy(rdma_seesion_state_name, "RDMA_WRITE_RUNNING");
			break;
		case 13 :
			strcpy(rdma_seesion_state_name, "RDMA_READ_RUNNING");
			break;
		case 14 :
			strcpy(rdma_seesion_state_name, "SEND_DONE");
			break;
		case 15 :
			strcpy(rdma_seesion_state_name, "RDMA_DONE");
			break;
		case 16 :
			strcpy(rdma_seesion_state_name, "RDMA_READ_ADV");
			break;
		case 17 :
			strcpy(rdma_seesion_state_name, "RDMA_WRITE_ADV");
			break;
		case 18 :
			strcpy(rdma_seesion_state_name, "CM_DISCONNECT");
			break;
		case 19 :
			strcpy(rdma_seesion_state_name, "ERROR");
			break;

		default :
			strcpy(rdma_seesion_state_name, "Un-defined state.");
			break;
	}

	return rdma_seesion_state_name;
}



/**
 * print the information of this scatterlist
 */
void  print_scatterlist_info(struct scatterlist* sl_ptr , int nents ){

  int i;

  printk(KERN_INFO "\n %s, %d entries , Start\n", __func__, nents);

  for(i=0; i< nents; i++){
   // if(sl_ptr[i] != NULL){   // for array[N], the item can't be NULL.
      printk(KERN_INFO "%s, \n page_link(struct page*) : 0x%lx \n  offset : 0x%x bytes\n  length : 0x%x bytes \n dma_addr : 0x%llx \n ",
                        __func__, sl_ptr[i].page_link, sl_ptr[i].offset, sl_ptr[i].length, sl_ptr[i].dma_address );
  //  }
  }

  printk(KERN_INFO " %s End\n\n", __func__);

}


/**
 * <<<<<<<<<<<<<<<<<<<<< End of Debug Functions <<<<<<<<<<<<<<<<<<<<<
 */





